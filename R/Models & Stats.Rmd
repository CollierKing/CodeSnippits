---
title: "Models & Stats"
output: html_notebook
---

Model references:
<a href="https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/?utm_content=bufferb2b9e&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer">(Model Overview)</a>


## ============================================================
## Statistical Testing

### Prop Tests (of unequal proportions)
```{r prop tests}
#Prop Test of Significance @ 99% Confidence
# of successes / # of attempts
ptestresult <- prop.test(c(allSummtbl$club_attrCount[j],allSummtbl$lounge_attrCount[j]),c(nrow(clubtbl),nrow(loungetbl)),correct = FALSE,conf.level=0.99)



```

### T-Tests (2 sided)
```{r t tests}

#Reverse (using summary stats without actual data vectors)

#plug in: mean, sd, n

ttestresult <- tsum.test(mean.x=allSummtbl$club_avgRating[j],   s.x=allSummtbl$club_sdRating[j], n.x=allSummtbl$club_attrCount[j],
          mean.y=allSummtbl$lounge_avgRating[j], s.y=allSummtbl$lounge_sdRating[j], n.y=allSummtbl$lounge_attrCount[j])

```


## ============================================================
## PCA & Factor Analysis

```{r pca & FA}

##Example 1 - Factor Analysis & PCA
#Data: Employee Satisfaction Survey

setwd("C:/Users/Collier/Dropbox/Skills/R/Data")

#Import Data
emp_factor <- read.csv("Employee-Factor.csv")
names(emp_factor)
#Use PCA to find number of factors

# ?princomp

pca <- princomp(emp_factor)

summary(pca)

plot(pca, type="lines")

biplot(pca,cex=c(0.75,1))

biplot(pca,scale.=TRUE,cex=c(0.5,0.75))

## Example 2 

library(stats)
pca <- princomp(train, cor = TRUE)
train_reduced  <- predict(pca,train)
test_reduced  <- predict(pca,test)

#Run exploratory factor analysis to find number of drivers

fact <- factanal(emp_factor,2,rotation="varimax")

fact

library(lattice)

biplot.factanal <- function (fa.fit,...)
{
        # Get the first two columns of scores, i.e.,
        # scores on first two factors
        x = fa.fit$scores[,1:2]
        # Get the loadings on the first two factors
        y = fa.fit$loadings[,1:2]
        biplot(x,y,...)
}


biplot.factanal(factanal(emp_factor,factors=2,scores="regression"),
                cex=c(0.5,0.8))


#Compute scores for each driver


fact.final <- factanal(emp_factor, 
                       5,
                       rotation="varimax",
                       score="regression")


#the factor score values

head(fact.final$scores)

```

## ============================================================
## Linear Regression

```{r lm}

#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays
x_train <- input_variables_values_training_datasets
y_train <- target_variables_values_training_datasets
x_test <- input_variables_values_test_datasets
x <- cbind(x_train,y_train)
# Train the model using the training sets and check score
linear <- lm(y_train ~ ., data = x)
summary(linear)
#Predict Output
predicted= predict(linear,x_test) 

```

## ============================================================
## Logistic Regression

```{r logit regression}
# odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
# ln(odds) = ln(p/(1-p))
# logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk

x <- cbind(x_train,y_train)
# Train the model using the training sets and check score
logistic <- glm(y_train ~ ., data = x,family='binomial')
summary(logistic)
#Predict Output
predicted= predict(logistic,x_test)

```

## ============================================================
## Decision Trees

```{r decision trees}

library(rpart)
x <- cbind(x_train,y_train)
# grow tree 
fit <- rpart(y_train ~ ., data = x,method="class")
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)


```

## ============================================================
## Support Vector Machine

```{r svm}

library(e1071)
x <- cbind(x_train,y_train)
# Fitting model
fit <-svm(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)


```

## ============================================================
## Naive Bayes

```{r naive bayes}

library(e1071)
x <- cbind(x_train,y_train)
# Fitting model
fit <-naiveBayes(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

```

### Naive Bays VS Linear Discriminant Analysis (LDA)

```{r nb vs lda}

library(caret)
attach(iris)

inTrain <- createDataPartition(y=iris$Species,p=0.7,list = FALSE)

training <- iris[inTrain,]
testing <- iris[-inTrain,]

modlda <- train(Species~.,data=training,method="lda")
modnb <- train(Species~.,data=training,method="nb")
plda <- predict(modlda,testing)
pnb <- predict(modnb,testing)

table(plda,pnb)

```
# =============================================================
# KNN (K-Nearest Neighbors)

```{r knn}

library(knn)
x <- cbind(x_train,y_train)
# Fitting model
fit <-knn(y_train ~ ., data = x,k=5)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)


```

# =============================================================
# K-Means Clustering

```{r kmeans}

library(cluster)
fit <- kmeans(X, 3) # 5 cluster solution

```


# =============================================================
# Random Forest

```{r random forest}

library(randomForest)
x <- cbind(x_train,y_train)
# Fitting model
fit <- randomForest(Species ~ ., x,ntree=500)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

```

# ===============================================================
# Gradient Boosting

```{r Gradient Boosting}

library(caret)
x <- cbind(x_train,y_train)
# Fitting model
fitControl <- trainControl( method = "repeatedcv", number = 4, repeats = 4)
fit <- train(y ~ ., data = x, method = "gbm", trControl = fitControl,verbose = FALSE)
predicted= predict(fit,x_test,type= "prob")[,2] 



```