{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mx\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"x\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%python\n",
    "dbutils.fs.ls(\"/tmp/\" + username + \"/ipCount.parquet\")\n",
    "\n",
    "%fs head /mnt/training/Chicago-Crimes-2018.csv\n",
    "\n",
    "println(dbutils.fs.head(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\", 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// csv\n",
    "val path = \"/mnt/training/EDGAR-Log-20170329/EDGAR-Log-20170329.csv\"\n",
    "\n",
    "val logDF = spark\n",
    "  .read\n",
    "  .option(\"header\", true)\n",
    "  .csv(path)\n",
    "  .sample(withReplacement=false, fraction=0.3, seed=3) // using a sample to reduce data size\n",
    "\n",
    "display(logDF)\n",
    "\n",
    "// parquet\n",
    "val ipCountDF2 = (spark\n",
    "  .read\n",
    "  .parquet(writePath)\n",
    "  .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "//tab delim\n",
    "display(spark.read\n",
    "  .option(\"delimiter\", \"\\t\")\n",
    "  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n",
    ")\n",
    "\n",
    "display(spark.read\n",
    "  .option(\"delimiter\", \"\\t\")\n",
    "  .option(\"header\", true)\n",
    "  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n",
    ")\n",
    "\n",
    "// infering schema\n",
    "val crimeDF = spark.read\n",
    "  .option(\"delimiter\", \"\\t\")\n",
    "  .option(\"header\", true)\n",
    "  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\")\n",
    "  .option(\"inferSchema\", true)\n",
    "  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n",
    "\n",
    "display(crimeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val AccessKey = \"AKIAJBRYNXGHORDHZB4A\"\n",
    "// Encode the Secret Key to remove any \"/\" characters\n",
    "val SecretKey = \"a0BzE1bSegfydr3%2FGE3LSPM6uIV5A4hOUfpH8aFF\".replace(\"/\", \"%2F\")\n",
    "val AwsBucketName = \"databricks-corp-training/common\"\n",
    "val MountName = \"/mnt/training-%s\".format(username)\n",
    "\n",
    "// unmount the bucket\n",
    "try {\n",
    "  dbutils.fs.unmount(s\"$MountName\") // Use this to unmount as needed\n",
    "} catch {\n",
    "  case ioe: java.rmi.RemoteException => println(s\"$MountName already unmounted\")\n",
    "}\n",
    "\n",
    "// mount the bucket\n",
    "try {\n",
    "  val MountTarget = \"s3a://%s:%s@%s\".format(AccessKey, SecretKey, AwsBucketName)\n",
    "  dbutils.fs.mount(MountTarget, MountName)\n",
    "} catch {\n",
    "  case ioe: java.rmi.RemoteException => println($\"$MountName already mounted. Run previous cells to unmount first\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jdbcHostname = \"server1.databricks.training\"\n",
    "val jdbcPort = 5432\n",
    "val jdbcDatabase = \"training\"\n",
    "\n",
    "val jdbcUrl = s\"jdbc:postgresql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}\"\n",
    "\n",
    "import java.util.Properties\n",
    "\n",
    "val connectionProperties = new Properties()\n",
    "connectionProperties.put(\"user\", \"readonly\")\n",
    "connectionProperties.put(\"password\", \"readonly\")\n",
    "\n",
    "val accountDF = spark.read.jdbc(url=jdbcUrl, table=\"Account\", properties=connectionProperties)\n",
    "display(accountDF)\n",
    "\n",
    "// calculate range of values\n",
    "import org.apache.spark.sql.functions.{min, max}\n",
    "\n",
    "val dfMin = accountDF.select(min(\"insertID\")).first()(0).asInstanceOf[Long]\n",
    "val dfMax = accountDF.select(max(\"insertID\")).first()(0).asInstanceOf[Long]\n",
    "\n",
    "println(s\"DataFrame minimum: $dfMin \\nDataFrame maximum: $dfMax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val accountDFParallel = spark.read.jdbc(\n",
    "  jdbcUrl,                      // the JDBC URL\n",
    "  \"Account\",                    // the name of the table\n",
    "  \"\"\"\"insertID\"\"\"\",             // the name of a column of an integral type that will be used for partitioning.\n",
    "  dfMin,                        // the minimum value of columnName used to decide partition stride.\n",
    "  dfMax,                        // the maximum value of columnName used to decide partition stride\n",
    "  12,                           // the number of partitions/connections\n",
    "  connectionProperties          // the connection properties\n",
    ")\n",
    "\n",
    "display(accountDFParallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(accountDF.rdd.getNumPartitions)\n",
    "// 1\n",
    "println(accountDFParallel.rdd.getNumPartitions)\n",
    "// 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeIt[T](op: => T): Float = {\n",
    "  val start = System.currentTimeMillis\n",
    "  val res = op\n",
    "  val end = System.currentTimeMillis\n",
    "  (end - start) / 1000f\n",
    "}\n",
    "\n",
    "val time1 = timeIt(accountDF.describe())\n",
    "val time2 = timeIt(accountDFParallel.describe())\n",
    "\n",
    "println(s\"Serial read completed in $time1 seconds vs $time2 seconds for parallel read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverErrorDF\n",
    "  .write\n",
    "  .mode(\"overwrite\") // overwrites a file if it already exists\n",
    "  .parquet(\"/tmp/\" + username + \"/log20170329/transformedLogs.parquet\")\n",
    "\n",
    "val writePath = \"/tmp/\" + username + \"/ipCount.parquet\"\n",
    "\n",
    "ipCountDF\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(writePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeRenamedColsDF.write.mode(\"overwrite\").parquet(\"/tmp/\" + username + \"/crime.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ANSWER\n",
    "\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    "\n",
    "val schema = StructType(List(\n",
    "  StructField(\"SMS\", StringType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val zipsDF = spark.read.json(\"/mnt/training/zips.json\")\n",
    "zipsDF.printSchema\n",
    "\n",
    "val zipsSchema = zipsDF.schema\n",
    "\n",
    "zipsSchema.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}\n",
    "\n",
    "// define a schema\n",
    "val zipsSchema2 = StructType(List(\n",
    "  StructField(\"city\", StringType, true),\n",
    "  StructField(\"pop\", IntegerType, true)\n",
    "))\n",
    "\n",
    "// apply a schema on read\n",
    "val zipsDF2 = spark.read\n",
    "  .schema(zipsSchema2)\n",
    "  .json(\"/mnt/training/zips.json\")\n",
    "\n",
    "display(zipsDF2)\n",
    "\n",
    "// nested data types\n",
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType, ArrayType, FloatType}\n",
    "\n",
    "val zipsSchema3 = StructType(List(\n",
    "  StructField(\"city\", StringType, true), \n",
    "  StructField(\"loc\", \n",
    "    ArrayType(FloatType, true), true),\n",
    "  StructField(\"pop\", IntegerType, true)\n",
    "))\n",
    "\n",
    "val zipsDF3 = (spark.read\n",
    "  .schema(zipsSchema3)\n",
    "  .json(\"/mnt/training/zips.json\")\n",
    ")\n",
    "display(zipsDF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    "\n",
    "val schema2 = StructType(List(\n",
    "  StructField(\"SMS\", StructType(List(\n",
    "    StructField(\"Address\", StringType, true),\n",
    "    StructField(\"date\", StringType, true),\n",
    "    StructField(\"metadata\", StructType(List(\n",
    "      StructField(\"name\", StringType, true)\n",
    "    )), true))\n",
    "  ), true)\n",
    "))\n",
    "\n",
    "val SMSDF2 = spark.read\n",
    "  .schema(schema2)\n",
    "  .json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n",
    "  .filter($\"SMS\".isNotNull)\n",
    "\n",
    "val cols = SMSDF2.columns(0)\n",
    "val schemaJson = SMSDF2.schema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, ArrayType, StringType, IntegerType, LongType}\n",
    "\n",
    "val path = \"/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4\"\n",
    "// val path = \"/mnt/training/twitter/firehose/2018/*/*/*/*\"\n",
    "\n",
    "val fullTweetSchema = StructType(List(\n",
    "  StructField(\"id\", LongType, true),\n",
    "  StructField(\"user\", StructType(List(\n",
    "    StructField(\"id\", LongType, true),\n",
    "    StructField(\"screen_name\", StringType, true),\n",
    "    StructField(\"location\", StringType, true),\n",
    "    StructField(\"friends_count\", IntegerType, true),\n",
    "    StructField(\"followers_count\", IntegerType, true),\n",
    "    StructField(\"description\", StringType, true)\n",
    "  )), true),\n",
    "  StructField(\"entities\", StructType(List(\n",
    "    StructField(\"hashtags\", ArrayType(\n",
    "      StructType(List(\n",
    "        StructField(\"text\", StringType, true)\n",
    "      ))\n",
    "    ), true),\n",
    "    StructField(\"urls\", ArrayType(\n",
    "      StructType(List(\n",
    "        StructField(\"url\", StringType, true),\n",
    "        StructField(\"expanded_url\", StringType, true),\n",
    "        StructField(\"display_url\", StringType, true)\n",
    "      ))\n",
    "    ), true) \n",
    "  )), true),\n",
    "  StructField(\"lang\", StringType, true),\n",
    "  StructField(\"text\", StringType, true),\n",
    "  StructField(\"created_at\", StringType, true)\n",
    "))\n",
    "\n",
    "val fullTweetDF = spark.read.schema(fullTweetSchema).json(path)\n",
    "fullTweetDF.printSchema()\n",
    "display(fullTweetDF)\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val schema = fullTweetSchema.fieldNames\n",
    "scala.util.Sorting.quickSort(schema)\n",
    "val tweetCount = fullTweetDF.filter(col(\"id\").isNotNull).count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrupt Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// not recommended\n",
    "\n",
    "val data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "val corruptDF = spark.read\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "  .json(sc.parallelize(data))\n",
    "\n",
    "display(corruptDF)\n",
    "\n",
    "try {\n",
    "  val data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "  val corruptDF = spark.read\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .json(sc.parallelize(data))\n",
    "\n",
    "  display(corruptDF)  \n",
    "  \n",
    "} catch {\n",
    "  case e:Exception => print(e)\n",
    "}\n",
    "\n",
    "// recommended (bad records path)\n",
    "\n",
    "val basePath = \"%s/etl1s\".format(userhome)\n",
    "val myBadRecords = \"%s/badRecordsPath\".format(basePath)\n",
    "\n",
    "println(\"\"\"Your temp directory is \"%s\"\"\"\".format(myBadRecords))\n",
    "println(\"-\"*80)\n",
    "\n",
    "val data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "val corruptDF = spark.read\n",
    "  .option(\"badRecordsPath\", myBadRecords)\n",
    "  .json(sc.parallelize(data))\n",
    "\n",
    "display(corruptDF)\n",
    "                                                             \n",
    "// view records in path\n",
    "                                                             \n",
    "val path = \"%s/*/*/*\".format(myBadRecords)\n",
    "display(spark.read.json(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val crimeDF = spark.read\n",
    "  .option(\"delimiter\", \"\\t\")\n",
    "  .option(\"header\", true)\n",
    "  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\")\n",
    "  .option(\"inferSchema\", true)\n",
    "  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n",
    "\n",
    "// remove spaces & invalid characters (camelCase)\n",
    "val cols = crimeDF.columns\n",
    "val camelCols = new scala.collection.mutable.ListBuffer[String]()\n",
    "cols.foreach(camelCols += _.toLowerCase.split(\" \").reduceLeft(_+_.capitalize))\n",
    "\n",
    "val crimeRenamedColsDF = crimeDF.toDF(camelCols:_*)\n",
    "display(crimeRenamedColsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// make lowercase & substitude characters\n",
    "import org.apache.spark.sql.functions.{col, lower, translate}\n",
    "\n",
    "val dupedWithColsDF = (dupedDF\n",
    "  .select(col(\"*\"),\n",
    "    lower(col(\"firstName\")).alias(\"lcFirstName\"),\n",
    "    lower(col(\"lastName\")).alias(\"lcLastName\"),\n",
    "    lower(col(\"middleName\")).alias(\"lcMiddleName\"),\n",
    "    translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "))\n",
    "display(dupedWithColsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ANSWER\n",
    "val accountDF = fullTweetFilteredDF.select(col(\"user.id\").alias(\"userID\"), \n",
    "    col(\"user.screen_name\").alias(\"screenName\"),\n",
    "    col(\"user.location\"),\n",
    "    col(\"user.friends_count\").alias(\"friendsCount\"),\n",
    "    col(\"user.followers_count\").alias(\"followersCount\"),\n",
    "    col(\"user.description\")\n",
    ")\n",
    "\n",
    "display(accountDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{unix_timestamp, col}\n",
    "import org.apache.spark.sql.types.TimestampType\n",
    "\n",
    "val timestampFormat = \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "val tweetDF = fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "  col(\"user.id\").alias(\"userID\"), \n",
    "  col(\"lang\").alias(\"language\"),\n",
    "  col(\"text\"),\n",
    "  unix_timestamp(col(\"created_at\"), timestampFormat).cast(TimestampType).alias(\"createdAt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//pass columns names with $ sign\n",
    "\n",
    "val serverErrorDF = logDF\n",
    "  .filter(($\"code\" >= 500) && ($\"code\" < 600))\n",
    "  .select(\"date\", \"time\", \"extention\", \"code\")\n",
    "\n",
    "display(serverErrorDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val fullTweetFilteredDF = fullTweetDF\n",
    "  .filter(col(\"id\").isNotNull)\n",
    "\n",
    "display(fullTweetFilteredDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// another dropping null example\n",
    "val corruptDF = Seq(\n",
    "  (Some(11), Some(66), Some(5)),\n",
    "  (Some(12), Some(68), None),\n",
    "  (Some(1), None, Some(6)),\n",
    "  (Some(2), Some(72), Some(7))\n",
    ").toDF(\"hour\", \"temperature\", \"wind\")\n",
    "\n",
    "display(corruptDF)\n",
    "\n",
    "val corruptDroppedDF = corruptDF.na.drop(\"any\")\n",
    "\n",
    "display(corruptDroppedDF)\n",
    "\n",
    "//fillna with HC'd values \n",
    "val map = Map(\"temperature\" -> 68, \"wind\" -> 6)\n",
    "val corruptImputedDF = corruptDF.na.fill(map)\n",
    "\n",
    "display(corruptImputedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-duping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val duplicateDF = Seq(\n",
    "  (15342, \"Conor\", \"red\"),\n",
    "  (15342, \"conor\", \"red\"),\n",
    "  (12512, \"Dorothy\", \"blue\"),\n",
    "  (5234, \"Doug\", \"aqua\")\n",
    "  ).toDF(\"id\", \"name\", \"favorite_color\")\n",
    "\n",
    "display(duplicateDF)\n",
    "\n",
    "val duplicateDedupedDF = duplicateDF.dropDuplicates(\"id\", \"favorite_color\")\n",
    "\n",
    "display(duplicateDedupedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// explode: split out nested column components\n",
    "import org.apache.spark.sql.functions.{explode, col}\n",
    "\n",
    "val hashtagDF = fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "    explode(col(\"entities.hashtags.text\")).alias(\"hashtag\")\n",
    ")\n",
    "\n",
    "val urlDF = (fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "      explode(col(\"entities.urls\")).alias(\"urls\"))\n",
    "  .select(\n",
    "      col(\"tweetID\"),\n",
    "      col(\"urls.url\").alias(\"URL\"),\n",
    "      col(\"urls.display_url\").alias(\"displayURL\"),\n",
    "      col(\"urls.expanded_url\").alias(\"expandedURL\"))\n",
    ")\n",
    "\n",
    "hashtagDF.show()\n",
    "urlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// normalizing numeric data\n",
    "\n",
    "import org.apache.spark.sql.functions.{col, max, min}\n",
    "\n",
    "val colMin = integerDF.select(min(\"id\")).first()(0).asInstanceOf[Long]\n",
    "val colMax = integerDF.select(max(\"id\")).first()(0).asInstanceOf[Long]\n",
    "\n",
    "val normalizedIntegerDF = integerDF\n",
    "  .withColumn(\"normalizedValue\", (col(\"id\") - colMin) / (colMax - colMin) )\n",
    "\n",
    "display(normalizedIntegerDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// other data manipulation functions\n",
    "// explode()\tReturns a new row for each element in the given array or map\n",
    "// pivot()\tPivots a column of the current DataFrame and perform the specified aggregation\n",
    "// cube()\tCreate a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them\n",
    "// rollup()\tCreate a multi-dimensional rollup for the current DataFrame using the specified columns, so we can run aggregation on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format\n",
    "\n",
    "// timestamp (dow)\n",
    "\n",
    "val pageviewsDF = spark.read\n",
    "  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/\")\n",
    "  .withColumn(\"dow\", date_format($\"timestamp\", \"u\").alias(\"dow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If/Then Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.desc\n",
    "\n",
    "val cols = smartphoneDF.columns.toSet\n",
    "val sample = smartphoneDF.orderBy(desc(\"Application\")).first().toSeq(0).toString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{from_utc_timestamp, hour}\n",
    "\n",
    "val countsDF = serverErrorDF\n",
    "  .select(hour(from_utc_timestamp($\"time\", \"GMT\")).alias(\"hour\"))\n",
    "  .groupBy($\"hour\")\n",
    "  .count()\n",
    "  .orderBy($\"hour\")\n",
    "\n",
    "display(countsDF)\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.functions.desc\n",
    "\n",
    "val ipCountDF = logDF\n",
    "  .select($\"ip\")\n",
    "  .groupBy($\"ip\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "\n",
    "display(ipCountDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggregatedDowDF = pageviewsEnhancedDF\n",
    "  .groupBy($\"dow\", $\"longName\", $\"abbreviated\", $\"shortName\")  \n",
    "  .sum(\"requests\")                                             \n",
    "  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n",
    "  .orderBy($\"dow\")\n",
    "\n",
    "display(aggregatedDowDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-partioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function\tTransformation Type\tUse\tEvenly distributes data across partitions?\n",
    "// .coalesce(n)\tnarrow (does not shuffle data)\treduce the number of partitions\tno\n",
    "// .repartition(n)\twide (includes a shuffle operation)\tincrease the number of partitions\tyes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeRenamedColsDF.repartition(1).write.mode(\"overwrite\").parquet(\"/tmp/\" + username + \"/crimeRepartitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wikiDF = spark.read\n",
    "  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet\")\n",
    "\n",
    "display(wikiDF)\n",
    "\n",
    "// show # partitions\n",
    "val partitions = wikiDF.rdd.getNumPartitions\n",
    "println(s\"Partitions: $partitions\")\n",
    "\n",
    "// re-partition to increase # of partitions\n",
    "val repartitionedWikiDF = wikiDF.repartition(16)\n",
    "println($\"Partitions: ${repartitionedWikiDF.rdd.getNumPartitions}\")\n",
    "\n",
    "// coalesce to reduce # of partitions\n",
    "val coalescedWikiDF = repartitionedWikiDF.coalesce(2)\n",
    "println($\"Partitions: ${coalescedWikiDF.rdd.getNumPartitions}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark uses a default value of 200 partitions, which comes from real-world experience by Spark engineers. This is an adjustable configuration setting. Run the following cell to see this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "\n",
    "// adjust the number of partitions, changes after a shuffle operation\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println($\"Partitions: ${coalescedWikiDF.orderBy(\"requests\").rdd.getNumPartitions}\")\n",
    "\n",
    "// The .orderBy() triggered the repartition of the DataFrame into 8 partitions. \n",
    "// Now reset the default value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd8.sc:1: not found: value spark\n",
      "val res8 = spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
      "           ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF (User-Defined Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define the function\n",
    "def manual_split: String => Seq[String] = _.split(\"e\")\n",
    "\n",
    "manual_split(\"this is my example string\")\n",
    "\n",
    "// register the function\n",
    "val manualSplitScalaUDF = spark.udf.register(\"manualSplitSQLUDF\", manual_split)\n",
    "\n",
    "// create random data\n",
    "import org.apache.spark.sql.functions.{sha1, rand}\n",
    "\n",
    "val randomDF = (spark.range(1, 10000 * 10 * 10 * 10)\n",
    "  .withColumn(\"random_value\", rand(seed=10).cast(\"string\"))\n",
    "  .withColumn(\"hash\", sha1($\"random_value\"))\n",
    "  .drop(\"random_value\")\n",
    ")\n",
    "\n",
    "display(randomDF)\n",
    "\n",
    "// apply the function on the dataframe\n",
    "val randomAugmentedDF = randomDF.select($\"*\", manualSplitScalaUDF($\"hash\").alias(\"augmented_col\"))\n",
    "\n",
    "display(randomAugmentedDF)\n",
    "\n",
    "// register the dataframe for sql queries\n",
    "randomDF.createOrReplaceTempView(\"randomTable\")\n",
    "\n",
    "%sql\n",
    "SELECT id,\n",
    "  hash,\n",
    "  manualSplitSQLUDF(hash) as augmented_col\n",
    "FROM\n",
    "  randomTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// register a new udf\n",
    "val plusOneUDF = spark.udf.register(\"plusOneUDF\", (input: Float) => input + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeIt[T](op: => T): Float = {\n",
    "  val start = System.currentTimeMillis\n",
    "  val res = op\n",
    "  val end = System.currentTimeMillis\n",
    "  (end - start) / 1000f\n",
    "}\n",
    "\n",
    "val time1 = timeIt(randomFloatsDF.withColumn(\"incremented_float\", plusOneUDF($\"random_float\")).count)\n",
    "val time2 = timeIt(randomFloatsDF.withColumn(\"incremented_float\", $\"random_float\" + 1).count)\n",
    "\n",
    "println(s\"UDF completed in $time1 seconds vs $time2 seconds for built-in functionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// another example\n",
    "def IPConvert(IPString: String): Long = {\n",
    "  val Array(a, b, c, d) = IPString.split(\"\\\\.\").map(_.toLong)\n",
    "  println(a, b, c, d)\n",
    "  (a * scala.math.pow(256, 3) + b * scala.math.pow(256, 2) + (c * 256) + d).toLong\n",
    "}\n",
    "\n",
    "IPConvert(\"1.2.3.4\") // should equal 16909060\n",
    "\n",
    "// ANSWER\n",
    "val IPConvertUDF = spark.udf.register(\"IPConvertUDF\", IPConvert _)\n",
    "// TEST - Run this cell to test your solution\n",
    "val testDF = Seq(\n",
    "  \"1.2.3.4\",\n",
    "  \"10.10.10.10\",\n",
    "  \"23.13.65.23\").toDF(\"ip\")\n",
    "\n",
    "val result = testDF.select(IPConvertUDF($\"ip\")).collect()\n",
    "\n",
    "// apply on a DF\n",
    "val IPDF = Seq(\"123.123.123.123\", \"1.2.3.4\", \"127.0.0.0\").toDF(\"ip\")\n",
    "\n",
    "val IPDFWithParsedIP = IPDF.withColumn(\"parsedIP\", IPConvertUDF($\"ip\"))\n",
    "\n",
    "display(IPDFWithParsedIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adv UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmanual_add\u001b[39m\n",
       "\u001b[36mres2_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_add(a: Int, b: Int): Int = a + b\n",
    "\n",
    "manual_add(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val manualAddScalaUDF = spark.udf.register(\"manualAddSQLUDF\", manual_add _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val integerDF = Seq(\n",
    "  (1, 2),\n",
    "  (3, 4),\n",
    "  (5, 6)\n",
    ").toDF(\"col1\", \"col2\")\n",
    "\n",
    "display(integerDF)\n",
    "\n",
    "val integerAddDF = integerDF.select($\"*\", manualAddScalaUDF($\"col1\", $\"col2\").alias(\"sum\"))\n",
    "\n",
    "display(integerAddDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Complex Output **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex outputs are helpful when you need to return multiple values from your UDF. The UDF design pattern involves returning a single column to drill down into, to pull out the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mMathOperations\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class MathOperations(sum: Float, multiplication: Float, division: Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmanual_math\u001b[39m\n",
       "\u001b[36mres4_1\u001b[39m: \u001b[32mMathOperations\u001b[39m = \u001b[33mMathOperations\u001b[39m(\u001b[32m3.0F\u001b[39m, \u001b[32m2.0F\u001b[39m, \u001b[32m0.5F\u001b[39m)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_math: (Int, Int) => \n",
    "MathOperations = (a, b) => \n",
    "MathOperations(a + b, a * b, a / b.asInstanceOf[Float])\n",
    "\n",
    "//sum,//mult,//divide\n",
    "\n",
    "manual_math(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// register\n",
    "val manualMathScalaUDF = spark.udf.register(\"manualMathSQLUDF\", manual_math)\n",
    " \n",
    "// apply\n",
    "display(integerDF.select($\"*\", manualMathScalaUDF($\"col1\", $\"col2\").alias(\"sum\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mtemperature\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define output class\n",
    "case class temperature(fahrenheit: Float, celsius: Float, kelvin: Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtemperatureConverter\u001b[39m\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mtemperature\u001b[39m = \u001b[33mtemperature\u001b[39m(\u001b[32m50.0F\u001b[39m, \u001b[32m10.0F\u001b[39m, \u001b[32m283.15F\u001b[39m)\n",
       "\u001b[36mres7_2\u001b[39m: \u001b[32mtemperature\u001b[39m = \u001b[33mtemperature\u001b[39m(\u001b[32m10.0F\u001b[39m, \u001b[32m-12.222223F\u001b[39m, \u001b[32m260.92776F\u001b[39m)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define function\n",
    "def temperatureConverter(temp: Float, unit: String): temperature = {\n",
    "  if (unit == \"C\") {\n",
    "    temperature((temp * 1.8f) + 32, temp, temp + 273.15f)\n",
    "  }\n",
    "  else {\n",
    "    val c = (temp - 32) * (5f / 9)\n",
    "    temperature(temp, c, c + 273.15f)\n",
    "  }\n",
    "}\n",
    "\n",
    "temperatureConverter(10, \"C\") // should be temperature(50.0, 10, 283.15)\n",
    "temperatureConverter(10, \"F\") // should be temperature(10, -12.2, 260.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// register udf\n",
    "val temperatureConverterUDF = udf((temp: Float, unit: String) => temperatureConverter(temp, unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST - Run this cell to test your solution\n",
    "import org.apache.spark.sql.types.{FloatType, StringType}\n",
    "\n",
    "dbTest(\"ET2-S-04-04-01\", Some(List(FloatType, StringType)), temperatureConverterUDF.inputTypes)\n",
    "\n",
    "println(\"Tests passed!\")\n",
    "\n",
    "// apply udf on df\n",
    "val weatherEnhancedDF = weatherDF.withColumn(\"TAVGAdjusted\", temperatureConverterUDF($\"TAVG\", $\"UNIT\"))\n",
    "\n",
    "display(weatherEnhancedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format\n",
    "\n",
    "val pageviewsDF = spark.read\n",
    "  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/\")\n",
    "  .withColumn(\"dow\", date_format($\"timestamp\", \"u\").alias(\"dow\"))\n",
    "\n",
    "display(pageviewsDF)\n",
    "\n",
    "val pageviewsEnhancedDF = pageviewsDF.join(labelsDF, \"dow\")\n",
    "\n",
    "display(pageviewsEnhancedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// show explain plan (how join was executed)\n",
    "\n",
    "aggregatedDowDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Spark did a broadcast join rather than a shuffle join. In other words, it broadcast labelsDF to the larger pageviewsDF, replicating the smaller DataFrame on each node of our cluster. This avoided having to move the larger DataFrame across the cluster.\n",
    "\n",
    "Take a look at the broadcast threshold by accessing the configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "println(\"Threshold: $threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// disable broadcasting\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now notice the lack of broadcast in the query physical plan.\n",
    "\n",
    "pageviewsDF.join(labelsDF, \"dow\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of telling Spark to explicitly broadcast tables. The first is to change the Spark configuration, which affects all operations. The second is to declare it using the broadcast() function in the functions package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// explicitly broadcast join\n",
    "import org.apache.spark.sql.functions.broadcast\n",
    "\n",
    "pageviewsDF.join(broadcast(labelsDF), \"dow\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// another example\n",
    "// ANSWER\n",
    "import org.apache.spark.sql.functions.broadcast\n",
    "\n",
    "val logWithIPEnhancedDF = (logWithIPDF\n",
    "  .join(broadcast(countryLookupDF), logWithIPDF.col(\"IPLookupISO2\") === countryLookupDF.col(\"alpha2Code\"))\n",
    "  .drop(\"alpha2Code\", \"alpha3Code\", \"numericCode\", \"ISO31662SubdivisionCode\", \"independentTerritory\")\n",
    ")\n",
    "\n",
    "display(logWithIPEnhancedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Database Writes**\n",
    "Database writes are the inverse of what was covered in Lesson 4 of ETL Part 1. In that lesson you defined the number of partitions in the call to the database.\n",
    "\n",
    "By contrast and when writing to a database, the number of active connections to the database is determined by the number of partitions of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiDF.write.mode(\"OVERWRITE\").parquet(userhome+\"/wiki.parquet\")\n",
    "\n",
    "%python\n",
    "for i in dbutils.fs.ls(userhome+\"/wiki.parquet\"):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Note on Upserts**\n",
    "\n",
    "Upserts insert a record into a database if it doesn't already exist, and updates the existing record if it does. Upserts are not supported in core Spark due to the transactional nature of upserting and the immutable nature of Spark. You can only append or overwrite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: **\n",
    "\n",
    "Import Helper Functions and Data\n",
    "A function is defined for you to print out the number of records in each DataFrame. Run the following cell to define that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Utility method to count & print the number of records in each partition\n",
    "def printRecordsPerPartition(df:org.apache.spark.sql.Dataset[Row]): Unit = {\n",
    "  println(\"Per-Partition Counts:\")\n",
    "  \n",
    "  val results = df.rdd                                   // Convert to an RDD\n",
    "    .mapPartitions(it => Array(it.size).iterator, true)  // For each partition, count\n",
    "    .collect()                                           // Return the counts to the driver\n",
    "\n",
    "  results.foreach(x => println(\"* \" + x))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ANSWER\n",
    "val wikiDF = spark.read\n",
    "  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet\")\n",
    "\n",
    "display(wikiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printRecordsPerPartition(wikiDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:**\n",
    "\n",
    "Repartition the Data\n",
    "Define three new DataFrames:\n",
    "\n",
    "wikiDF1Partition: wikiDF with 1 partition\n",
    "wikiDF16Partition: wikiDF with 16 partitions\n",
    "wikiDF128Partition: wikiDF with 128 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wikiDF1Partition = wikiDF.repartition(1)\n",
    "val wikiDF16Partition = wikiDF.repartition(16)\n",
    "val wikiDF128Partition = wikiDF.repartition(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**\n",
    "\n",
    "Examine the Distribution of Records\n",
    "Use printRecordsPerPartition() to examine the distribution of records across the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printRecordsPerPartition(wikiDF1Partition)\n",
    "printRecordsPerPartition(wikiDF16Partition)\n",
    "printRecordsPerPartition(wikiDF128Partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:**\n",
    "\n",
    "Coalesce wikiDF16Partition and Examine the Results\n",
    "Coalesce wikiDF16Partition to 10 partitions, saving the result to wikiDF16PartitionCoalesced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wikiDF16PartitionCoalesced = wikiDF16Partition.coalesce(10)\n",
    "\n",
    "wikiDF16PartitionCoalesced.rdd.getNumPartitions)\n",
    "\n",
    "printRecordsPerPartition(wikiDF16PartitionCoalesced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization of Data Storage with Managed and Unmanaged Tables**\n",
    "\n",
    "A **managed table** is a table that manages both the data itself as well as the metadata. In this case, a DROP TABLE command removes both the metadata for the table as well as the data itself.\n",
    "\n",
    "**Unmanaged tables** manage the metadata from a table such as the schema and data location, but the data itself sits in a different location, often backed by a blob store like the Azure Blob or S3. Dropping an unmanaged table drops only the metadata associated with the table while the data itself remains in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to unmanaged table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create df\n",
    "val df = spark.range(1, 100)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// register the table\n",
    "df.write.mode(\"OVERWRITE\").saveAsTable(\"myTableManaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// describe table \n",
    "%sql\n",
    "DESCRIBE EXTENDED myTableManaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// writing to unmanaged table\n",
    "df.write.mode(\"OVERWRITE\").option(\"path\", userhome+\"/myTableUnmanaged\").saveAsTable(\"myTableUnmanaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED myTableUnmanaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// look at files backing up the managed table\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/\" + databaseName + \".db/mytablemanaged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// drop the managed table\n",
    "%sql\n",
    "DROP TABLE myTableManaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// after you drop a managed table it will \n",
    "// delete the underlying data\n",
    "// this will throw an error\n",
    "\n",
    "try {\n",
    "  display(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/\" + databaseName + \".db/mytablemanaged\"))\n",
    "  \n",
    "} catch {\n",
    "  case e:Exception => println(e)\n",
    "}\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/user/\" + username + \"/myTableUnmanaged\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// drop the unmanaged table\n",
    "\n",
    "%sql\n",
    "DROP TABLE myTableUnmanaged\n",
    "\n",
    "// the data is still there\n",
    "\n",
    "%python\n",
    "dbutils.fs.ls(\"dbfs:/user/\" + username + \"/myTableUnmanaged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, ArrayType, StringType, IntegerType, LongType}\n",
    "\n",
    "// define the schema\n",
    "lazy val fullTweetSchema = StructType(List(\n",
    "  StructField(\"id\", LongType, true),\n",
    "  StructField(\"user\", StructType(List(\n",
    "    StructField(\"id\", LongType, true),\n",
    "    StructField(\"screen_name\", StringType, true),\n",
    "    StructField(\"location\", StringType, true),\n",
    "    StructField(\"friends_count\", IntegerType, true),\n",
    "    StructField(\"followers_count\", IntegerType, true),\n",
    "    StructField(\"description\", StringType, true)\n",
    "  )), true),\n",
    "  StructField(\"entities\", StructType(List(\n",
    "    StructField(\"hashtags\", ArrayType(\n",
    "      StructType(List(\n",
    "        StructField(\"text\", StringType, true)\n",
    "      ))\n",
    "    ), true),\n",
    "    StructField(\"urls\", ArrayType(\n",
    "      StructType(List(\n",
    "        StructField(\"url\", StringType, true),\n",
    "        StructField(\"expanded_url\", StringType, true),\n",
    "        StructField(\"display_url\", StringType, true)\n",
    "      ))\n",
    "    ), true) \n",
    "  )), true),\n",
    "  StructField(\"lang\", StringType, true),\n",
    "  StructField(\"text\", StringType, true),\n",
    "  StructField(\"created_at\", StringType, true)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// read and apply\n",
    "val path = \"/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4\"\n",
    "// val path = \"/mnt/training/twitter/firehose/2018/*/*/*/*\" // This imports of the data\n",
    "val tweetDF = spark.read\n",
    "  .schema(fullTweetSchema)\n",
    "  .json(path)\n",
    "  .filter($\"id\".isNotNull)\n",
    "\n",
    "display(tweetDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write udf to parse urls\n",
    "def getDomain(URL: String) : String = {\n",
    "  val pattern = \"\"\"https?://(www\\.)?([^/#\\?]+)\"\"\".r\n",
    "  pattern.findFirstMatchIn(URL).map(_.group(2)).getOrElse(\"\")\n",
    "}\n",
    "\n",
    "getDomain(\"https://www.databricks.com/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// test the UDF\n",
    "val urls = List(\n",
    "  \"https://www.databricks.com/\",\n",
    "  \"https://databricks.com/\",\n",
    "  \"https://databricks.com/training-overview/training-self-paced\",\n",
    "  \"http://www.databricks.com/\",\n",
    "  \"http://databricks.com/\",\n",
    "  \"http://databricks.com/training-overview/training-self-paced\",\n",
    "  \"http://www.apache.org/\",\n",
    "  \"http://spark.apache.org/docs/latest/\"\n",
    ")\n",
    "\n",
    "urls.map(getDomain).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// register the udf \n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val getDomainUDF = sqlContext.udf.register(\"getDomainUDF\", getDomain _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// apply the udf\n",
    "\n",
    "// create the udf\n",
    "import org.apache.spark.sql.functions.explode\n",
    "\n",
    "val urlDF = (tweetDF\n",
    "  .withColumn(\"URL\", explode($\"entities.urls.expanded_url\"))\n",
    "  .select(\"URL\", \"created_at\") \n",
    "  .withColumn(\"parsedURL\", getDomainUDF($\"URL\"))\n",
    ")\n",
    "\n",
    "display(urlDF)\n",
    "\n",
    "val cols = urlDF.columns\n",
    "val sample = urlDF.first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// parse timestamp\n",
    "import org.apache.spark.sql.functions.{unix_timestamp, hour}\n",
    "import org.apache.spark.sql.types.TimestampType\n",
    "\n",
    "val timestampFormat = \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "val urlWithTimestampDF = urlDF\n",
    "  .withColumn(\"timestamp\", unix_timestamp($\"created_at\", timestampFormat).cast(TimestampType).alias(\"createdAt\"))\n",
    "  .drop(\"created_at\")\n",
    "  .withColumn(\"hour\", hour($\"timestamp\"))\n",
    "\n",
    "display(urlWithTimestampDF)\n",
    "\n",
    "val cols = urlWithTimestampDF.columns\n",
    "val sample = urlWithTimestampDF.first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// aggregate and sort\n",
    "import org.apache.spark.sql.functions.desc\n",
    "\n",
    "val urlTrendsDF = (urlWithTimestampDF\n",
    "  .groupBy(\"hour\", \"parsedURL\")\n",
    "  .count()\n",
    "  .orderBy($\"hour\", desc(\"count\"))\n",
    "  .limit(10)\n",
    ")\n",
    "\n",
    "display(urlTrendsDF)\n",
    "\n",
    "val cols = urlTrendsDF.columns\n",
    "val sample = urlTrendsDF.first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// join new data\n",
    "\n",
    "// import new table\n",
    "val badActorsDF = spark.read.parquet(\"/mnt/training/twitter/supplemental/badactors.parquet\")\n",
    "\n",
    "display(badActorsDF)\n",
    "\n",
    "val cols = badActorsDF.columns\n",
    "val sample = badActorsDF.first\n",
    "\n",
    "// add a column for bad actors\n",
    "val tweetWithMaliciousDF = tweetDF\n",
    "  .join(badActorsDF, tweetDF.col(\"user.id\") === badActorsDF.col(\"userID\"), \"left\")\n",
    "  .withColumn(\"maliciousAcct\", $\"userID\".isNotNull)\n",
    "  .drop(\"screen_name\", \"userID\")\n",
    "\n",
    "display(tweetWithMaliciousDF)\n",
    "\n",
    "val cols = tweetWithMaliciousDF.columns\n",
    "val sample = tweetWithMaliciousDF.first\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load records\n",
    "// ANSWER\n",
    "urlTrendsDF.repartition(4).write.mode(\"overwrite\").parquet(userhome + \"/tmp/urlTrends.parquet\")\n",
    "tweetWithMaliciousDF.repartition(4).write.mode(\"overwrite\").parquet(userhome + \"/tmp/tweetWithMaliciousDF.parquet\")\n",
    "\n",
    "val urlTrendsDFTemp = spark.read.parquet(userhome + \"/tmp/urlTrends.parquet\")\n",
    "val tweetWithMaliciousDFTemp = spark.read.parquet(userhome + \"/tmp/tweetWithMaliciousDF.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// select\n",
    "%sql\n",
    "SELECT * FROM People10M\n",
    "\n",
    "// describe\n",
    "%sql\n",
    "DESCRIBE People10M\n",
    "\n",
    "// filter\n",
    "%sql\n",
    "SELECT firstName, middleName, lastName, birthDate\n",
    "FROM People10M\n",
    "WHERE year(birthDate) > 1990 AND gender = 'F'\n",
    "\n",
    "%sql\n",
    "SELECT firstName, middleName, lastName, year(birthDate) as birthYear, salary \n",
    "FROM People10M\n",
    "WHERE year(birthDate) > 1990 AND gender = 'F'\n",
    "\n",
    "%sql\n",
    "SELECT year(birthDate) as birthYear, count(*) AS total\n",
    "FROM People10M\n",
    "WHERE firstName = 'Mary' AND gender = 'F'\n",
    "GROUP BY birthYear\n",
    "ORDER BY birthYear\n",
    "\n",
    "%sql\n",
    "SELECT year(birthDate) as birthYear,  firstName, count(*) AS total\n",
    "FROM People10M\n",
    "WHERE (firstName = 'Dorothy' or firstName = 'Donna') AND gender = 'F' AND year(birthDate) > 1990\n",
    "GROUP BY birthYear, firstName\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create temporary view\n",
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW TheDonnas AS\n",
    "  SELECT * \n",
    "  FROM People10M \n",
    "  WHERE firstName = 'Donna'\n",
    "\n",
    "%sql\n",
    "SELECT * FROM TheDonnas\n",
    "\n",
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW WomenBornAfter1990 AS\n",
    "  SELECT firstName, middleName, lastName, year(birthDate) AS birthYear, salary \n",
    "  FROM People10M\n",
    "  WHERE year(birthDate) > 1990 AND gender = 'F'\n",
    "\n",
    "%sql\n",
    "SELECT birthYear, count(*) \n",
    "FROM WomenBornAfter1990 \n",
    "WHERE firstName = 'Mary' \n",
    "GROUP BY birthYear \n",
    "ORDER BY birthYear\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS junk;\n",
    "\n",
    "USE junk;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS IPGeocode\n",
    "  USING parquet\n",
    "  OPTIONS (\n",
    "    path \"dbfs:/mnt/training/ip-geocode.parquet\"\n",
    "  )\n",
    "\n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS BikeSharingDay\n",
    "  USING csv\n",
    "  OPTIONS (\n",
    "    path \"/mnt/training/bikeSharing/data-001/day.csv\",\n",
    "    inferSchema \"true\",\n",
    "    header \"true\"\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS DatabricksBlog\n",
    "  USING json\n",
    "  OPTIONS (\n",
    "    path \"dbfs:/mnt/training/databricks-blog.json\",\n",
    "    inferSchema \"true\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// pulling nested data\n",
    "%sql\n",
    "SELECT dates.createdOn, dates.publishedOn \n",
    "FROM DatabricksBlog\n",
    "\n",
    "%sql\n",
    "SELECT title, \n",
    "       cast(dates.publishedOn AS timestamp) AS publishedOn \n",
    "FROM DatabricksBlog\n",
    "\n",
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW DatabricksBlog2 AS\n",
    "  SELECT *, \n",
    "         cast(dates.publishedOn AS timestamp) AS publishedOn \n",
    "  FROM DatabricksBlog\n",
    "\n",
    "%sql\n",
    "SELECT title, \n",
    "       date_format(publishedOn, \"MMM dd, yyyy\") AS date, \n",
    "       link \n",
    "FROM DatabricksBlog2\n",
    "WHERE year(publishedOn) = 2013\n",
    "ORDER BY publishedOn\n",
    "\n",
    "//col:authors has the nested list of authors\n",
    "//col:author has each specific auther in list\n",
    "%sql\n",
    "SELECT title, \n",
    "       authors, \n",
    "       explode(authors) AS author, \n",
    "       link \n",
    "FROM DatabricksBlog \n",
    "WHERE size(authors) > 1 \n",
    "ORDER BY title\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// lateral views\n",
    "\n",
    "// use LATERAL VIEW to explode multiple columns at once, in this case, \n",
    "// the columns authors and categories\n",
    "%sql\n",
    "SELECT dates.publishedOn, title, author, category\n",
    "FROM DatabricksBlog\n",
    "LATERAL VIEW explode(authors) exploded_authors_view AS author\n",
    "LATERAL VIEW explode(categories) exploded_categories AS category\n",
    "WHERE title = \"Apache Spark 1.1: The State of Spark Streaming\"\n",
    "ORDER BY author, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
