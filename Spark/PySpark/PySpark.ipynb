{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FS\n",
    "# If you run out of space, use %fs rm -r /tmp/ to recursively (and permanently) remove all items from a directory.\n",
    "\n",
    "%python\n",
    "dbutils.fs.ls(\"/tmp/\" + username + \"/ipCount.parquet\")\n",
    "\n",
    "#inspect the head of a file\n",
    "%fs head /mnt/training/Chicago-Crimes-2018.csv\n",
    "print(dbutils.fs.head('/mnt/training/UbiqLog4UCI/14_F/log_1-6-2014.txt', 200))\n",
    "\n",
    "#show all files in the workspace\n",
    "path = \"/mnt/training/twitter/firehose/2018/01/10/01\"\n",
    "display(dbutils.fs.ls(path)) # %fs ls evaluates to this\n",
    "\n",
    "#or \n",
    "\n",
    "%fs\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a csv to spark df\n",
    "path = \"/mnt/training/EDGAR-Log-20170329/EDGAR-Log-20170329.csv\"\n",
    "\n",
    "logDF = (spark\n",
    "  .read\n",
    "  .option(\"header\", True)\n",
    "  .csv(path)\n",
    "  .sample(withReplacement=False, fraction=0.3, seed=3) # using a sample to reduce data size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV\n",
    "#example with options (tuples)\n",
    "crimeDF = (spark.read\n",
    "  .option(\"delimiter\", \"\\t\") #what type of file?\n",
    "  .option(\"header\", True) #spark does not auto infer header\n",
    "  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\") #convert timestamp\n",
    "  .option(\"inferSchema\", True) #Set \"inferSchema\" to True, which triggers Spark to make an extra pass over the data to infer the schema.\n",
    "  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n",
    ")\n",
    "\n",
    "## JSON\n",
    "#example reading in multiple files\n",
    "path = \"mnt/training/UbiqLog4UCI/14_F/log*\"\n",
    "smartphoneDF = spark.read.json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n",
    "\n",
    "# smartphoneDF = (spark\n",
    "#   .read\n",
    "#   .option(\"header\", True)\n",
    "#   .csv(path)\n",
    "# #   .sample(withReplacement=False, fraction=0.3, seed=3) # using a sample to reduce data size\n",
    "# )\n",
    "\n",
    "#display\n",
    "display(logDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##S3\n",
    "#connecting to S3\n",
    "ACCESS_KEY = \"\"\n",
    "# Encode the Secret Key to remove any \"/\" characters\n",
    "SECRET_KEY = \"Z%2FZ\".replace(\"/\", \"%2F\") #in-practice keep secure\n",
    "AWS_BUCKET_NAME = \"databricks-corp-training/common\" #how it will appear in dbfs\n",
    "MOUNT_NAME = \"/mnt/training-{}\".format(username)\n",
    "\n",
    "# n practice, always secure your AWS credentials. \n",
    "# Do this by either maintaining a single notebook with \n",
    "# restricted permissions that holds AWS keys, or delete the \n",
    "# cells or notebooks that expose the keys. After a cell used to \n",
    "# mount a bucket is run, access this mount in any notebook, any \n",
    "# cluster, and share the mount between colleagues.\n",
    "\n",
    "#mount the bucket\n",
    "try:\n",
    "    MOUNT_TARGET = \"s3a://{}:{}@{}\".format(ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME)\n",
    "    dbutils.fs.mount(MOUNT_TARGET, MOUNT_NAME)\n",
    "except:\n",
    "    print(\"{} already mounted. Run previous cells to unmount first\".format(MOUNT_NAME))\n",
    "\n",
    "#unmount the bucket\n",
    "try:\n",
    "    dbutils.fs.unmount(MOUNT_NAME) # Use this to unmount as needed\n",
    "except:\n",
    "    print(\"{} already unmounted\".format(MOUNT_NAME))\n",
    "\n",
    "#explore the mount in filesystem\n",
    "%fs ls /mnt/<MOUNT_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##JDBC\n",
    "# Connecting to JDBC\n",
    "# create connection url\n",
    "jdbcHostname = \"server1.databricks.training\"\n",
    "jdbcPort = 5432\n",
    "jdbcDatabase = \"training\"\n",
    "\n",
    "jdbcUrl = \"jdbc:postgresql://{0}:{1}/{2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "#define connection properties\n",
    "connectionProps = {\n",
    "  \"user\": \"readonly\",\n",
    "  \"password\": \"readonly\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SERIAL read into df from database\n",
    "accountDF = spark.read.jdbc(\n",
    "    url=jdbcUrl, \n",
    "    table=\"Account\", \n",
    "    properties=connectionProps\n",
    ")\n",
    "display(accountDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARALLEL read into df from database\n",
    "accountDFParallel = spark.read.jdbc(\n",
    "  url=jdbcUrl, \n",
    "  table=\"Account\",\n",
    "  column='\"insertID\"', #partition column *use single quotes to avoid bug\n",
    "  lowerBound=dfMin, #needed if column set\n",
    "  upperBound=dfMax, #needed if column set\n",
    "  numPartitions=12,\n",
    "  properties=connectionProps\n",
    ")\n",
    "\n",
    "#Comparing performance\n",
    "#print # of partitions\n",
    "print(accountDF.rdd.getNumPartitions())\n",
    "print(accountDFParallel.rdd.getNumPartitions())\n",
    "\n",
    "#gather stats on both serial and parallel\n",
    "%timeit accountDF.describe()\n",
    "# loops, best of 3: 4.39 s per loop\n",
    "%timeit accountDFParallel.describe()\n",
    "# loops, best of 3: 2.67 s per loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An advantage of Parquet is that, unlike a CSV file which is normally a single file, Parquet is distributed so each partition of data in the cluster writes to its own \"part\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write df to parquet\n",
    "(serverErrorDF\n",
    "  .write\n",
    "  .mode(\"overwrite\") # overwrites a file if it already exists\n",
    "  .parquet(\"/tmp/\" + username + \"/log20170329/serverErrorDF.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeRenamedColsDF.write.mode(\"overwrite\").parquet(\"/tmp/\" + username + \"/crime.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED myTableManaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SCHEMAS\n",
    "# Providing a schema increases performance two to three times\n",
    "# Schema Inference\n",
    "zipsDF = spark.read.json(\"/mnt/training/zips.json\")\n",
    "zipsDF.printSchema()\n",
    "# root\n",
    "#  |-- _id: string (nullable = true)\n",
    "#  |-- city: string (nullable = true)\n",
    "#  |-- loc: array (nullable = true)\n",
    "#  |    |-- element: double (containsNull = true)\n",
    "#  |-- pop: long (nullable = true)\n",
    "#  |-- state: string (nullable = true)\n",
    "\n",
    "\n",
    "zipsSchema = zipsDF.schema\n",
    "print(type(zipsSchema))\n",
    "\n",
    "[field for field in zipsSchema]\n",
    "\n",
    "# tore the schema as an object by calling .schema on a DataFrame. \n",
    "# Schemas consist of a StructType, which is a collection of StructFields. \n",
    "# Each StructField gives a name and a type for a given field in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Defined Schemas\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "#create the schema\n",
    "zipsSchema2 = StructType([\n",
    "  StructField(\"city\", StringType(), True), \n",
    "  StructField(\"pop\", IntegerType(), True) \n",
    "])\n",
    "\n",
    "zipsDF2 = (spark.read\n",
    "  .schema(zipsSchema2)\n",
    "  .json(\"/mnt/training/zips.json\")\n",
    ")\n",
    "\n",
    "display(zipsDF2)\n",
    "\n",
    "# A primitive type contains the data itself.  The most common primitive types include:\n",
    "\n",
    "# | Numeric | General | Time |\n",
    "# |-----|-----|\n",
    "# | `FloatType` | `StringType` | `TimestampType` | \n",
    "# | `IntegerType` | `BooleanType` | `DateType` | \n",
    "# | `DoubleType` | `NullType` | |\n",
    "# | `LongType` | | |\n",
    "# | `ShortType` |  | |\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType\n",
    "\n",
    "#create the schema\n",
    "zipsSchema3 = StructType([\n",
    "  StructField(\"city\", StringType(), True), \n",
    "  StructField(\"loc\", \n",
    "    ArrayType(FloatType(), True), True),\n",
    "  StructField(\"pop\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "#apply the schema when reading in file\n",
    "zipsDF3 = (spark.read\n",
    "  .schema(zipsSchema3)\n",
    "  .json(\"/mnt/training/zips.json\")\n",
    ")\n",
    "display(zipsDF3)\n",
    "\n",
    "\n",
    "#apply UD Schema to files\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "schema2 = StructType([\n",
    "  StructField(\"SMS\", StructType([\n",
    "    StructField(\"Address\",StringType(),True),\n",
    "    StructField(\"date\",StringType(),True),\n",
    "    StructField(\"metadata\", StructType([\n",
    "      StructField(\"name\",StringType(), True)\n",
    "    ]), True),\n",
    "  ]), True)\n",
    "])\n",
    "\n",
    "SMSDF2 = (spark.read\n",
    "  .schema(schema2)\n",
    "  .json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n",
    "  .filter(col(\"SMS\").isNotNull()))\n",
    "\n",
    "display(SMSDF2)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrupt Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three different options for handling corrupt records set through the ParseMode option:\n",
    "\n",
    "# ParseMode\tBehavior\n",
    "# PERMISSIVE\tIncludes corrupt records in a \"_corrupt_record\" column (by default)\n",
    "# DROPMALFORMED\tIgnores all corrupted records\n",
    "# FAILFAST\tThrows an exception when it meets corrupted records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permissive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "corruptDF = (spark.read\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "  .json(sc.parallelize(data))\n",
    ")\n",
    "\n",
    "display(corruptDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "corruptDF = (spark.read\n",
    "  .option(\"mode\", \"DROPMALFORMED\")\n",
    "  .json(sc.parallelize(data))\n",
    ")\n",
    "display(corruptDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failfast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "    corruptDF = (spark.read\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .json(sc.parallelize(data))\n",
    "    )\n",
    "    display(corruptDF)\n",
    "  \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Records Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databricks has a special feature where you can specify where corrupt records should be saved for further inspection\n",
    "basePath = \"{}/etl1p\".format(userhome)\n",
    "myBadRecords = \"{}/badRecordsPath\".format(basePath)\n",
    "\n",
    "print(\"\"\"Your temp directory is \"{}\" \"\"\".format(myBadRecords))\n",
    "\n",
    "data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "corruptDF = (spark.read\n",
    "  .option(\"badRecordsPath\", myBadRecords)\n",
    "  .json(sc.parallelize(data))\n",
    ")\n",
    "display(corruptDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) read in the data specifiying the corrupt column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "SMSCorruptDF = (spark.read\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .option(\"columnNameOfCorruptRecord\", \"SMSCorrupt\")\n",
    "  .json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n",
    "  .select(\"SMSCorrupt\", \"SMS\")\n",
    "  .filter(col(\"SMSCorrupt\").isNotNull())\n",
    ")\n",
    "\n",
    "display(SMSCorruptDF)\n",
    "\n",
    "# 2) Use the badRecordsPath option to save corrupt \n",
    "# records to the directory specified by the corruptPath variable below.\n",
    "\n",
    "corruptPath = \"{}/corruptSMS\".format(basePath)\n",
    "\n",
    "SMSCorruptDF2 = (spark.read\n",
    "  .option(\"badRecordsPath\", corruptPath)\n",
    "  .json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n",
    ")\n",
    "\n",
    "display(SMSCorruptDF2)\n",
    "\n",
    "# 3) clean up temp files\n",
    "dbutils.fs.rm(basePath, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n",
    "\n",
    "corruptDF = (spark.read\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "  .json(sc.parallelize(data))\n",
    ")\n",
    "\n",
    "display(corruptDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptDF = spark.createDataFrame([\n",
    "  (11, 66, 5),\n",
    "  (12, 68, None),\n",
    "  (1, None, 6),\n",
    "  (2, 72, 7)], \n",
    "  [\"hour\", \"temperature\", \"wind\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPDF = spark.createDataFrame(\n",
    "    [\n",
    "    [\"123.123.123.123\"], \n",
    "    [\"1.2.3.4\"], \n",
    "    [\"127.0.0.0\"]\n",
    "    ]\n",
    "    , ['ip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hash vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sha1, rand\n",
    "randomDF = (spark.range(1, 10000 * 10 * 10 * 10)\n",
    "  .withColumn(\"random_value\", rand(seed=10).cast(\"string\"))\n",
    "  .withColumn(\"hash\", sha1(\"random_value\"))\n",
    "  .drop(\"random_value\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting nested fields \n",
    "accountDF = fullTweetFilteredDF.select(\n",
    "  col(\"user.id\").alias(\"userID\"), \n",
    "  col(\"user.screen_name\").alias(\"screenName\"), \n",
    "  col(\"user.location\").alias(\"location\"), \n",
    "  col(\"user.friends_count\").alias(\"friendsCount\"), \n",
    "  col(\"user.followers_count\").alias(\"followersCount\"), \n",
    "  col(\"user.description\").alias(\"description\")\n",
    ")\n",
    "\n",
    "display(accountDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupedDF = (dupedWithColsDF\n",
    "  .drop(\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter & select\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "serverErrorDF = (logDF\n",
    "  .filter((col(\"code\") >= 500) & (col(\"code\") < 600))\n",
    "  .select(\"date\", \"time\", \"extention\", \"code\")\n",
    ")\n",
    "\n",
    "display(serverErrorDF)\n",
    "\n",
    "#group by & agg\n",
    "from pyspark.sql.functions import from_utc_timestamp, hour, col\n",
    "\n",
    "countsDF = (serverErrorDF\n",
    "  .select(hour(from_utc_timestamp(col(\"time\"), \"GMT\")).alias(\"hour\"))\n",
    "  .groupBy(\"hour\")\n",
    "  .count()\n",
    "  .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "display(countsDF)\n",
    "\n",
    "#read and filter\n",
    "fullTweetFilteredDF = (\n",
    "  spark\n",
    "  .read\n",
    "  .schema(fullTweetSchema)\n",
    "  .json(path)\n",
    ").filter(col(\"id\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptDroppedDF = corruptDF.dropna(\"any\")\n",
    "\n",
    "display(corruptDroppedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute/Fill Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptImputedDF = corruptDF.na.fill({\"temperature\": 168, \"wind\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateDedupedDF = duplicateDF.dropDuplicates([\"id\", \"favorite_color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max, min\n",
    "\n",
    "dupedWithColsDF = (dupedDF\n",
    "  .select(col(\"*\"),\n",
    "    lower(col(\"firstName\")).alias(\"lcFirstName\"),\n",
    "    lower(col(\"lastName\")).alias(\"lcLastName\"),\n",
    "    lower(col(\"middleName\")).alias(\"lcMiddleName\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max, min\n",
    "from pyspark.sql.functions import col, lower, translate\n",
    "\n",
    "dupedWithColsDF = (dupedDF\n",
    "  .select(col(\"*\"),\n",
    "    translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If/Then Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #if/then conditional logic using the when() function and its .otherwise() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count records in DF\n",
    "dfCount = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple aggregation on a column (min/max)\n",
    "from pyspark.sql.functions import min,max\n",
    "\n",
    "# TODO\n",
    "dfMin = accountDF.select(min('insertID')).first()[0]\n",
    "dfMax = accountDF.select(max('insertID')).first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by ip and count times\n",
    "ipCountDF = (logDF\n",
    "  .select(from_utc_timestamp(col(\"time\"), \"GMT\").alias(\"time\"),col(\"ip\"))\n",
    "  .groupBy(\"ip\")\n",
    "  .count().alias(\"count\")\n",
    "  .orderBy(\"count\",ascending=False)\n",
    "  )\n",
    "  ## or...\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "ipCountDF = (logDF\n",
    "  .select(from_utc_timestamp(col(\"time\"), \"GMT\").alias(\"time\"),col(\"ip\"))\n",
    "  .groupBy(\"ip\")\n",
    "  .count().alias(\"count\")\n",
    "  .orderBy(desc(\"count\"))\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping by multiple columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "aggregatedDowDF = (pageviewsEnhancedDF\n",
    "  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n",
    "  .sum(\"requests\")                                             \n",
    "  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n",
    "  .orderBy(col(\"dow\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What appears to the user as a single DataFrame is actually data distributed across a cluster. \n",
    "# Each cluster holds partitions, or parts, of the data. \n",
    "# By repartitioning, we define how many different parts of our data to have.\n",
    "\n",
    "(crimeRenamedColsDF\n",
    " .repartition(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .parquet(\"/tmp/\" + username + \"/crimeRepartitioned.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see how many files we have now\n",
    "%python\n",
    "dbutils.fs.ls(\"/tmp/\" + username + \"/crimeRepartitioned.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlTrendsDF.repartition(4).write.mode(\"overwrite\").parquet(userhome + \"/tmp/urlTrends.parquet\")\n",
    "\n",
    "tweetWithMaliciousDF.repartition(4).write.mode(\"overwrite\").parquet(userhome + \"/tmp/tweetWithMaliciousDF.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get # Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlTrendsDFTemp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function\tUse\n",
    "* explode()\tReturns a new row for each element in the given array or map\n",
    "* pivot()\tPivots a column of the current DataFrame and perform the specified aggregation\n",
    "* cube()\tCreate a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them\n",
    "* rollup()\tCreate a multi-dimensional rollup for the current DataFrame using the specified columns, so we can run aggregation on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "# TODO\n",
    "hashtagDF = fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "    explode(col(\"entities.hashtags.text\")).alias(\"hashtag\")\n",
    ")\n",
    "\n",
    "urlDF = (fullTweetFilteredDF.select(col(\"id\").alias(\"tweetID\"), \n",
    "    explode(col(\"entities.urls\")).alias(\"urls\"))\n",
    "    .select(\n",
    "    col(\"tweetID\"),\n",
    "    col(\"urls.url\").alias(\"URL\"),\n",
    "    col(\"urls.display_url\").alias(\"displayURL\"),\n",
    "    col(\"urls.expanded_url\").alias(\"expandedURL\"))\n",
    ")\n",
    "\n",
    "hashtagDF.show()\n",
    "urlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "urlDF = (tweetDF\n",
    "  .withColumn(\"URL\", explode(\"entities.urls.expanded_url\"))\n",
    "  .select(\"URL\", \"created_at\") \n",
    "  .withColumn(\"parsedURL\", getDomainUDF(\"URL\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF (User-Defined Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is my ', 'xampl', ' string']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_split(x):\n",
    "    return x.split(\"e\")\n",
    "\n",
    "manual_split(\"this is my example string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# When you registered the UDF, it was named manualSplitSQLUDF for access in the SQL API. \n",
    "# This gives us the same access to the UDF you had in the python DataFrames API.\n",
    "\n",
    "manualSplitPythonUDF = \\# A name for access in Python (manualSplitPythonUDF)\n",
    "spark.udf.register(\n",
    "    \"manualSplitSQLUDF\", # A name for access in SQL (manualSplitSQLUDF)\n",
    "    manual_split, # The function itself (manual_split)\n",
    "    StringType() # The return type for the function (StringType)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excecute the UDF in a select\n",
    "randomAugmentedDF = \\\n",
    "randomDF.select(\"*\", \n",
    "                manualSplitPythonUDF(\"hash\").alias(\"augmented_col\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view for SQL\n",
    "randomDF.createOrReplaceTempView(\"randomTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT id,\n",
    "  hash,\n",
    "  manualSplitSQLUDF(hash) as augmented_col\n",
    "FROM\n",
    "  randomTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "  \n",
    "plusOneUDF = spark.udf.register(\"plusOneUDF\", lambda x: x + 1, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function\n",
    "def IPConvert(IPString):\n",
    "  A, B, C, D = [int(i) for i in IPString.split(\".\")]\n",
    "  return A*256**3 + B*256**2 + C*256 + D\n",
    "\n",
    "IPConvert(\"1.2.3.4\") # should equal 16909060\n",
    "\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "#register function as UDF\n",
    "IPConvertUDF = spark.udf.register(\n",
    "    \"IPConvertUDF\", # A name for access in SQL (manualSplitSQLUDF)\n",
    "    IPConvert, # The function itself (manual_split)\n",
    "    LongType() # The return type for the function (StringType)\n",
    ")\n",
    "\n",
    "# Run this cell to test your solution\n",
    "testDF = spark.createDataFrame((\n",
    "  (\"1.2.3.4\", ),\n",
    "  (\"10.10.10.10\", ),\n",
    "  (\"23.13.65.23\", )\n",
    "), (\"ip\",))\n",
    "\n",
    "result = [i[0] for i in testDF.select(IPConvertUDF(\"ip\")).collect()]\n",
    "\n",
    "IPDF = spark.createDataFrame([[\"123.123.123.123\"], [\"1.2.3.4\"], [\"127.0.0.0\"]], ['ip'])\n",
    "\n",
    "display(IPDF)\n",
    "\n",
    "IPDFWithParsedIP  = \\\n",
    "IPDF.select(\"*\", \n",
    "                IPConvertUDF(\"ip\").alias(\"parsedIP\")\n",
    "               )\n",
    "\n",
    "display(IPDFWithParsedIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode & Apply UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "urlDF = (tweetDF\n",
    "  .withColumn(\"URL\", explode(\"entities.urls.expanded_url\"))\n",
    "  .select(\"URL\", \"created_at\") \n",
    "  .withColumn(\"parsedURL\", getDomainUDF(\"URL\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, hour\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# TODO\n",
    "timestampFormat = \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "urlWithTimestampDF = (urlDF\n",
    "  .withColumn(\"timestamp\", \n",
    "              unix_timestamp(\"created_at\", \n",
    "                             timestampFormat).cast(TimestampType()\n",
    "                                                  ).alias(\"createdAt\")\n",
    "             )\n",
    "  .drop(\"created_at\")\n",
    "  .withColumn(\"hour\", hour(\"timestamp\"))\n",
    ")\n",
    "\n",
    "display(urlWithTimestampDF)\n",
    "\n",
    "#\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "urlTrendsDF = (urlWithTimestampDF\n",
    "  .groupBy(\"hour\", \"parsedURL\")\n",
    "  .count()\n",
    "  .orderBy(\"hour\", desc(\"count\"))\n",
    "  .limit(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adv UDF (Advanced User-Defined Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this takes 2 inputs\n",
    "def manual_add(x, y):\n",
    "    return x + y\n",
    "\n",
    "manual_add(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "manualAddPythonUDF = spark.udf.register(\"manualAddSQLUDF\", manual_add, IntegerType())\n",
    "\n",
    "integerDF = (spark.createDataFrame([\n",
    "  (1, 2),\n",
    "  (3, 4),\n",
    "  (5, 6)\n",
    "], [\"col1\", \"col2\"]))\n",
    "\n",
    "integerAddDF = integerDF.select(\"*\", \n",
    "                                manualAddPythonUDF(\"col1\", \"col2\").alias(\"sum\")\n",
    "                               )\n",
    "\n",
    "display(integerAddDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a schema for the return values\n",
    "from pyspark.sql.types import FloatType, StructType, StructField\n",
    "\n",
    "mathOperationsSchema = StructType([\n",
    "  StructField(\"sum\", FloatType(), True), \n",
    "  StructField(\"multiplication\", FloatType(), True), \n",
    "  StructField(\"division\", FloatType(), True) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 2.0, 0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this returns a tuple of 3 values\n",
    "\n",
    "def manual_math(x, y):\n",
    "    return (float(x + y), float(x * y), x / float(y))\n",
    "\n",
    "manual_math(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register UDF\n",
    "manualMathPythonUDF = spark.udf.register(\"manualMathSQLUDF\", manual_math, mathOperationsSchema)\n",
    "\n",
    "#compute/return results\n",
    "display(integerDF.select(\"*\", manualMathPythonUDF(\"col1\", \"col2\").alias(\"sum\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3 (Vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "#Use the decorator syntax to designate a Pandas UDF. \n",
    "#The input and outputs are both Pandas series of doubles.\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR) \n",
    "def pandas_plus_one(v):\n",
    "    return v + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "df = spark.range(0, 10 * 1000 * 1000)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "display(df.withColumn('id_transformed', pandas_plus_one(\"id\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4 (Return Multiple Items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define schema\n",
    "schema = StructType([\n",
    "  StructField(\"fahrenheit\", FloatType(), True), \n",
    "  StructField(\"celsius\", FloatType(), True), \n",
    "  StructField(\"kelvin\", FloatType(), True) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def temperatureConverter(temperature, unit):\n",
    "    if unit == \"C\":\n",
    "        c = temperature\n",
    "        f = (temperature * (9. / 5)) + 32\n",
    "    else:\n",
    "        f = temperature\n",
    "        c = (temperature - 32) * (5. / 9)\n",
    "    return (f, c, c + 273.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50.0, 10, 283.15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatureConverter(10, \"C\") # should be (50.0, 10, 283.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, -12.222222222222223, 260.92777777777775)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatureConverter(10, \"F\") # should be (10, -12.2, 260.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register udf\n",
    "temperatureConverterUDF = udf(temperatureConverter, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function\n",
    "weatherEnhancedDF = weatherDF.withColumn(\"TAVGAdjusted\", temperatureConverterUDF(\"TAVG\", \"UNIT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show result\n",
    "result = weatherEnhancedDF.select(\"TAVGAdjusted\").first()[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shuffle join shuffles data between nodes in a cluster. By contrast, a broadcast join moves the smaller of two DataFrames to where the larger DataFrame sits, minimizing the overall data transfer. By default, Spark performs a broadcast join if the total number of records is below a certain threshold. The threshold can be manually specified or you can manually specify that a broadcast join should take place. Since the automatic determination of whether a shuffle join should take place is by number of records, this could mean that really wide data would take up significantly more space per record and should therefore be specified manually.\n",
    "\n",
    "https://docs.databricks.com/delta/join-performance/index.html#join-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExplainPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregatedDowDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BroadcastJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Spark did a broadcast join rather than a shuffle join. In other words, it broadcast labelsDF to the larger pageviewsDF, replicating the smaller DataFrame on each node of our cluster. This avoided having to move the larger DataFrame across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadcastjoin\n",
    "pageviewsEnhancedDF = pageviewsDF.join(labelsDF, \"dow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the broadcastjoin threshold\n",
    "\n",
    "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(\"Threshold: {0:,}\".format( int(threshold) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify broadcast join in the actual join \n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "pageviewsDF.join(broadcast(labelsDF), \"dow\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# TODO\n",
    "logWithIPEnhancedDF = (logWithIPDF\n",
    "  .join(broadcast(countryLookupDF), \n",
    "        logWithIPDF.IPLookupISO2 == countryLookupDF.alpha2Code) #keys\n",
    "  .drop(\"alpha2Code\", \"alpha3Code\", \"numericCode\", \"ISO31662SubdivisionCode\", \"independentTerritory\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex2 (Join & Flag when not null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "tweetWithMaliciousDF = (tweetDF\n",
    "  .join(badActorsDF, tweetDF.user.id == badActorsDF.userID, \"left\")\n",
    "  .withColumn(\"maliciousAcct\", col(\"userID\").isNotNull())\n",
    "  .drop(\"screen_name\", \"userID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShuffleJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to a database in Spark differs from other tools largely due to its distributed nature. There are a number of variables that can be tweaked to optimize performance, largely relating to how data is organized on the cluster. Partitions are the first step in understanding performant database connections.\n",
    "\n",
    "A partition is a portion of your total data set, which is divided into many of these portions so Spark can distribute your work across a cluster.\n",
    "\n",
    "The other concept needed to understand Spark's computation is a slot (also known as a core). A slot/core is a resource available for the execution of computation in parallel. In brief, a partition refers to the distribution of data while a slot refers to the distribution of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Partitions\n",
    "\n",
    "https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\n",
    "\n",
    "In the context of JDBC database writes, the number of partitions determine the number of connections used to push data through the JDBC API. There are two ways to control this parallelism:\n",
    "\n",
    "Function\t| Transformation Type\t| Use\tEvenly | distributes data across partitions?\n",
    "\n",
    ".coalesce(n)\t| narrow (does not shuffle data)\t| reduce the number of partitions\t| no\n",
    "\n",
    ".repartition(n)\t| wide (includes a shuffle operation)\t| increase the number of partitions | yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wikiDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2c2b81f1825e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get number of partitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikiDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wikiDF' is not defined"
     ]
    }
   ],
   "source": [
    "# get number of partitions\n",
    "partitions = wikiDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase the number of partitions\n",
    "repartitionedWikiDF = wikiDF.repartition(16)\n",
    "\n",
    "#reduce the number of partitions\n",
    "coalescedWikiDF = repartitionedWikiDF.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#records per partition\n",
    "def countInPartition(iterator): \n",
    "    yield __builtin__.sum(1 for _ in iterator)\n",
    "    \n",
    "    results = (df.rdd                   # Convert to an RDD\n",
    "    .mapPartitions(countInPartition)  # For each partition, count\n",
    "    .collect()                        # Return the counts to the driver\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Default Partitions\n",
    "Spark uses a default value of 200 partitions, which comes from real-world experience by Spark engineers. This is an adjustable configuration setting. Run the following cell to see this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print default number of partitions\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "#This changes the number of partitions after a shuffle operation.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Database Writes\n",
    "\n",
    "when writing to a database, the number of active connections to the database is determined by the number of partitions of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 stages were initially triggered, one for each partition of our data. \n",
    "# When you repartitioned the DataFrame to 12 partitions, 12 stages were needed, \n",
    "# one to write each partition of the data. Run the following and observe how the \n",
    "# repartitioning changes the number of stages.\n",
    "\n",
    "wikiDF.repartition(12).write.mode(\"OVERWRITE\").parquet(userhome+\"/wiki.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Management\n",
    "\n",
    "## Managed vs Unmanaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#managed\n",
    "df.write.mode(\"OVERWRITE\").saveAsTable(\"myTableManaged\")\n",
    "\n",
    "#unmanaged\n",
    "df.write.mode(\"OVERWRITE\").option('path', userhome+'/myTableUnmanaged').saveAsTable(\"myTableUnmanaged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP TABLE myTableManaged\n",
    "#see managed remnants (returns null)\n",
    "display(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/\" + databaseName + \".db/mytablemanaged\"))\n",
    "\n",
    "DROP TABLE myTableUnmanaged\n",
    "#see unmanaged remnants\n",
    "dbutils.fs.ls(\"dbfs:/user/\" + username + \"/myTableUnmanaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "462px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
